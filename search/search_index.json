{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NeoGPT","text":""},{"location":"#welcome-to-neogpt","title":"Welcome to NeoGPT! \ud83d\udcda\ud83e\udd16\u2728","text":"<p>Welcome to NeoGPT, your open-source, locally-run Language Model (LLM) \ud83d\udcda, revolutionizing the way you interact with documents, YouTube videos, and more.</p> <p>Tired of tedious encounters with lengthy documents and endless YouTube videos? NeoGPT is here to transform your experience into one that's seamless, intelligent, and effortlessly conversational. Whether you're a professional, developer, researcher, or an enthusiastic learner, NeoGPT is your trusted companion, unlocking a new era of interactions with local files and online content. Say hello to NeoGPT, where intelligent conversations meet your world of information.</p> <p></p> <p>Join our Discord community to stay up to date with the latest developments and to share your feedback.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li> <p>Local Execution: Enjoy NeoGPT's services on your local system, ensuring data privacy and security.</p> </li> <li> <p>YouTube Videos: It supports YouTube videos, allowing you to interact with your favorite videos in a whole new way.</p> </li> <li> <p>User-Friendly: Command-line interface (CLI) is designed to be user-friendly and accessible to users with varying technical expertise.</p> </li> </ul>"},{"location":"#project-roadmap","title":"Project Roadmap","text":"<ul> <li> <p> RAG (Question Answering with local files) \ud83d\udcc2</p> </li> <li> <p> Chat with Youtube Videos \ud83c\udfa5</p> </li> <li> <p> Web Based RAG (Search on Web and local files) \ud83c\udf10\ud83d\udcc2</p> </li> <li> <p> Hybrid RAG (Keyword based and Semmantic Search) \ud83d\udd75\ufe0f\u200d\u2642\ufe0f\ud83d\udcc2</p> </li> <li> <p> FAISS Support \ud83d\udcca</p> </li> <li> <p> Chromadb Support \ud83c\udfb5</p> </li> <li> <p> Build a user-friendly CLI \u2328\ufe0f</p> </li> <li> <p> Upgrade Builder to support multiple file types including URLs \ud83d\udce6\ud83c\udf10</p> </li> <li> <p> User Interface \ud83d\udcbb (Streamlit)</p> </li> <li> <p> Chat with SQL DB \ud83e\udd16</p> </li> <li> <p> Support for other search engines (DuckDuckGo) \ud83d\udd0d</p> </li> <li> <p> Add support for other LLM types (Ollama) \ud83e\udde0</p> </li> <li> <p> Add other database support (MongoDB, ElasticSearch, etc.) \ud83d\udcc1\ud83d\udd0d</p> </li> <li> <p> Docker Support \ud83d\udc33</p> </li> <li> <p> Better Documentation \ud83d\udcd6</p> </li> <li> <p> Agent based chatbot \ud83e\udd16</p> </li> </ul> <p>Release Note</p> <p>Once the roadmap is complete by 75%, we will release the v0.1.0 of NeoGPT \ud83d\ude80\ud83e\udd16\u2728</p> <p>Note</p> <p>The roadmap is subject to change. We will update the roadmap as we progress.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions to NeoGPT! If you have ideas for new features or improvements, please open an issue or submit a pull request. For more information, see our contributing guide.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p> <p>Feedback Note</p> <p>NeoGPT is continuously evolving. Your feedback shapes its future. We're excited about the future of NeoGPT and appreciate your support on this journey! \ud83d\ude80\ud83e\udd16\u2728</p>"},{"location":"builder/","title":"Builder \ud83d\udc77","text":""},{"location":"builder/#building-database","title":"Building Database \ud83d\udee0\ufe0f","text":"<p>Builder is a script to build the vector database for NeoGPT. Currently it support 2 vector databases:</p> <ul> <li> <p>Chroma</p> </li> <li> <p>FAISS</p> </li> </ul>"},{"location":"builder/#specifying-the-database-type-cli","title":"Specifying the Database Type (CLI)","text":"<p>By default, the builder script will build a database using the Chroma database type. To build a database using the FAISS database type, run the builder script with the <code>--db</code> flag:</p> <p>To build a database using the Chroma database type (default): Terminal<pre><code>   python neogpt/builder.py --db Chroma\n</code></pre></p> <p>To build a database using the FAISS database type: Terminal<pre><code>   python neogpt/builder.py --db FAISS\n</code></pre></p>"},{"location":"builder/#supported-file-formats","title":"Supported file formats","text":"<p>The NeoGPT database builder supports a range of document formats, each associated with a specific loader that processes and includes the content from these documents in the database. The following document formats are supported:</p>"},{"location":"builder/#general-document-formats","title":"General Document Formats:","text":"<ul> <li> <p>.pdf (PDF): PDFMinerLoader</p> </li> <li> <p>.txt (Text): TextLoader</p> </li> <li> <p>.csv (CSV): CSVLoader</p> </li> <li> <p>.html (HTML): UnstructuredHTMLLoader</p> </li> <li> <p>.tsv (TSV): UnstructuredTSVLoader</p> </li> <li> <p>.eml (Email): UnstructuredEmailLoader</p> </li> <li> <p>.epub (eBook): UnstructuredEPubLoader</p> </li> <li> <p>.xls (Excel): UnstructuredExcelLoader</p> </li> <li> <p>.xlsx (Excel): UnstructuredExcelLoader</p> </li> <li> <p>.pptx (PowerPoint): UnstructuredPowerPointLoader</p> </li> <li> <p>.ppt (PowerPoint): UnstructuredPowerPointLoader</p> </li> <li> <p>.docx (Word Document): UnstructuredWordDocumentLoader</p> </li> <li> <p>.doc (Word Document): UnstructuredWordDocumentLoader</p> </li> <li> <p>.md (Markdown): UnstructuredMarkdownLoader</p> </li> <li> <p>.json (JSON): JSONLoader</p> </li> </ul>"},{"location":"builder/#chat-documents","title":"Chat Documents:","text":"<ul> <li>.txt (Whatsapp): WhatsAppLoader</li> </ul>"},{"location":"builder/#code-documents","title":"Code Documents:","text":"<ul> <li> <p>.py (Python): Language.PYTHON</p> </li> <li> <p>.cpp (C++): Language.CPP</p> </li> <li> <p>.go (Go): Language.GO</p> </li> <li> <p>.java (Java): Language.JAVA</p> </li> <li> <p>.kt (Kotlin): Language.KOTLIN</p> </li> <li> <p>.js (JavaScript): Language.JS</p> </li> <li> <p>.ts (TypeScript): Language.TS</p> </li> <li> <p>.php (PHP): Language.PHP</p> </li> <li> <p>.proto (Protocol Buffer): Language.PROTO</p> </li> <li> <p>.rst (reStructuredText): Language.RST</p> </li> <li> <p>.ruby (Ruby): Language.RUBY</p> </li> <li> <p>.rs (Rust): Language.RUST</p> </li> <li> <p>.scala (Scala): Language.SCALA</p> </li> <li> <p>.swift (Swift): Language.SWIFT</p> </li> <li> <p>.markdown (Markdown): Language.MARKDOWN</p> </li> <li> <p>.latex (LaTeX): Language.LATEX</p> </li> <li> <p>.html (HTML): Language.HTML</p> </li> <li> <p>.sol (Solidity): Language.SOL</p> </li> <li> <p>.cs (C#): Language.CSHARP</p> </li> <li> <p>.cobol (COBOL): Language.COBOL</p> </li> </ul> <p>Note</p> <p>For programming languages, we have been implemented to read the file content and store it as a document in the database. This ensures that the content of code files is processed appropriately, allowing NeoGPT to understand and respond to code-related queries.</p>"},{"location":"builder/#test-data","title":"Test data","text":"<p>For testing purposes, we have included 2 papers and 1 youtube video in the <code>neogpt/documents</code> folder.</p> <p>The database is built using 2 papers and 1 youtube video:</p> <ul> <li> <p>Attention Is All You Need</p> </li> <li> <p>HuggingGPT</p> </li> <li> <p>22 AI News EXPLAINED!!!</p> </li> </ul>"},{"location":"builder/#folder-structure","title":"Folder Structure","text":"Folder Structure<pre><code>neogpt\n   \u251c\u2500\u2500 builder.py\n   \u2502   \u251c\u2500\u2500 db\n   \u2502   \u2502   \u251c\u2500\u2500 chroma/\n   \u2502   \u2502   \u2514\u2500\u2500 faiss/\n   \u2502   \u251c\u2500\u2500 logs\n   \u2502   \u2502   \u251c\u2500\u2500 builder.log\n   \u2502   \u251c\u2500\u2500 modules\n   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n   \u2502   \u2502   \u251c\u2500\u2500 load_chats.py\n   \u2502   \u2502   \u251c\u2500\u2500 load_code.py\n   \u2502   \u2502   \u251c\u2500\u2500 load_docs.py\n   \u2502   \u2502   \u2514\u2500\u2500 load_web.py\n</code></pre>"},{"location":"builder/#adding-your-own-data","title":"Adding Your Own Data","text":"<p>To add your own data to the NeoGPT database, follow these steps:</p> <ol> <li> <p>Prepare your local documents or content in one of the supported formats mentioned above. (Multiple formats can be used at the same time.)</p> </li> <li> <p>Place your data in the <code>neogpt/documents</code> folder within your NeoGPT project directory.</p> </li> <li> <p>Run the builder script by executing the following command in your terminal or command prompt:</p> </li> </ol> Terminal<pre><code>   python neogpt/builder.py\n</code></pre> <p>Note</p> <p>It will take some time to build the database depending on the number of documents and the size of the documents.</p> <p>To add youtube videos, refer below section</p> <p>A folder named <code>neogpt/db</code> will be created in your NeoGPT project directory. This folder will contain the database files.</p>"},{"location":"builder/#adding-urls-to-the-database","title":"Adding URL's to the Database","text":"<p>To include URL's in the NeoGPT database, follow these steps:</p> <ol> <li> <p>Create a file <code>builder.url</code> in the <code>neogpt/documents</code> folder within the project directory.</p> </li> <li> <p>Add the URLs to the <code>builder.url</code> file, one URL per line.     Example:     builder.url<pre><code>  https://www.youtube.com/watch?v=VideoID1\n  https://www.youtube.com/watch?v=VideoID2\n  https://neokd.github.io/NeoGPT/\n</code></pre>    You can include URLs from various sources, not limited to YouTube videos.</p> </li> <li> <p>Run the builder script by executing the following command in your terminal or command prompt:</p> </li> </ol> Terminal<pre><code>python neogpt/builder.py\n</code></pre> <p>If you want to extract the child pages of a URL (for non-YouTube URLs), you can use the <code>--recursive</code> flag. The builder script uses <code>WebBaseLoader</code> by default, which extracts only the root or specified domain of the URL. To extract child pages, run the builder script with the <code>--recursive</code> flag:</p> Terminal<pre><code>python neogpt/builder.py --recursive\n</code></pre>"},{"location":"builder/#adding-chat-data-to-the-database","title":"Adding Chat Data to the Database","text":""},{"location":"builder/#whatsapp","title":"WhatsApp","text":"<p>To add WhatsApp chat data to the NeoGPT database, follow these steps:</p> <ol> <li> <p>Export the chat from WhatsApp.</p> </li> <li> <p>Move the exported chat file to the <code>neogpt/documents</code> folder within the project directory.</p> </li> <li> <p>Run the builder script by executing the following command in your terminal or command prompt:</p> </li> </ol> Terminal<pre><code>python neogpt/builder.py\n</code></pre> <p>Tip</p> <p>You can add all the supported file formats to the <code>neogpt/documents</code> folder and run the builder script. The builder script will automatically detect the file format and process the content accordingly.</p>"},{"location":"installation/","title":"Installation \ud83e\uddd1\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb","text":"<p>This guide will walk you through the process of setting up NeoGPT on your system. NeoGPT is designed to be user-friendly and is compatible with a variety of operating systems.</p>"},{"location":"installation/#pre-requisites","title":"Pre-requisites","text":"<p>Before you begin, make sure you have the following prerequisites installed on your system:</p> <ul> <li> <p>Python 3.10 or higher</p> </li> <li> <p>Git</p> </li> <li> <p>Editor of your choice (VSCode, PyCharm, etc.)</p> </li> </ul>"},{"location":"installation/#installation-steps","title":"Installation Steps","text":"<p>You can install NeoGPT by following whichever of the following methods is most convenient for you:</p> <ol> <li> <p>Bash Script</p> </li> <li> <p>Docker</p> </li> <li> <p>Poetry</p> </li> <li> <p>pip</p> </li> <li> <p>conda</p> </li> </ol>"},{"location":"installation/#with-bash-script","title":"With bash script","text":"<p>It is recommended to use the bash script to install NeoGPT. The bash script will automatically install all the required packages and set up the virtual environment for you. Followed by that it will build the database and run the CLI.</p> <p>Recommended</p> <ol> <li> <p>Clone the repository using the following command Terminal<pre><code>git clone https://github.com/neokd/NeoGPT.git\n</code></pre></p> </li> <li> <p>Navigate to the root directory of the repository Terminal<pre><code>cd NeoGPT\n</code></pre></p> </li> <li> <p>Run the following command in your terminal or command prompt: Terminal<pre><code>bash ./install.sh\n</code></pre></p> </li> </ol> <p>Note</p> <p>The script will not create any virtual environment. It will use the default python environment. If you want to use a virtual environment, you can create one and activate it before running the script.</p>"},{"location":"installation/#with-docker","title":"With docker","text":"<p>Docker</p> <p>You need to have docker installed on your system to use this method. You can install docker by following the instructions here.</p> <ol> <li> <p>Clone the repository using the following command Terminal<pre><code>git clone https://github.com/neokd/NeoGPT.git\n</code></pre></p> </li> <li> <p>Navigate to the root directory of the repository Terminal<pre><code>cd NeoGPT\n</code></pre></p> </li> <li> <p>Run the following command in your terminal or command prompt: Terminal<pre><code>docker compose up --build\n</code></pre></p> </li> <li> <p>Run the following command in your terminal or command prompt: Terminal<pre><code>docker compose up\n</code></pre></p> </li> <li> <p>You can now access the UI at http://localhost:8501</p> </li> </ol>"},{"location":"installation/#with-poetry","title":"With poetry","text":"<p>Poetry</p> <p>You need to have poetry installed on your system to use this method. You can install poetry by running the following command in your terminal or command prompt: Terminal<pre><code>pip install poetry\n</code></pre></p> <ol> <li> <p>Clone the repository using the following command Terminal<pre><code>git clone https://github.com/neokd/NeoGPT.git\n</code></pre></p> </li> <li> <p>Navigate to the root directory of the repository Terminal<pre><code>cd NeoGPT\n</code></pre></p> </li> <li> <p>Run the following command in your terminal or command prompt: Terminal<pre><code>poetry install\n</code></pre></p> </li> <li> <p>Run the following command in your terminal or command prompt: Terminal<pre><code>    poetry run python main.py\n</code></pre></p> </li> </ol>"},{"location":"installation/#with-pip","title":"With pip","text":"<ol> <li> <p>Clone the repository using the following command Terminal<pre><code>git clone https://github.com/neokd/NeoGPT.git\n</code></pre></p> </li> <li> <p>Navigate to the root directory of the repository Terminal<pre><code>cd NeoGPT\n</code></pre></p> </li> <li> <p>Set up a virtual environment and activate it</p> <p>For Windows: Terminal<pre><code>    python -m venv neogpt-env\n    neogpt-env\\Scripts\\activate\n</code></pre> For Linux/MacOS:</p> Terminal<pre><code>    python3 -m venv neogpt-env\n    source neogpt-env/bin/activate\n</code></pre> </li> <li> <p>Install the required packages Terminal<pre><code>pip install -r requirements.txt\n</code></pre></p> </li> </ol>"},{"location":"installation/#with-conda","title":"With conda","text":"<ol> <li> <p>Clone the repository using the following command Terminal<pre><code>git clone https://github.com/neokd/NeoGPT.git\n</code></pre></p> </li> <li> <p>Navigate to the root directory of the repository Terminal<pre><code>cd NeoGPT\n</code></pre></p> </li> <li> <p>Set up a virtual environment and activate it Terminal<pre><code>conda create -n neogpt-env python=3.10\nconda activate neogpt-env\n</code></pre></p> </li> <li> <p>Install the required packages Terminal<pre><code>pip install -r requirements.txt\n</code></pre></p> </li> </ol>"},{"location":"installation/#llamacpp","title":"LLamaCpp","text":"<p>NeoGPT used llama-cpp-python to load the LLM models.</p> <p>If you want to use BLAS or Metal with llama-cpp you can set appropriate flags:</p> <p>for BLAS:</p> Terminal<pre><code>CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.11\n</code></pre> <p>for Metal (macOS only):</p> Terminal<pre><code>CMAKE_ARGS=\"-DLLAMA_METAL=on\"  FORCE_CMAKE=1 pip install llama-cpp-python==0.2.11 --no-cache-dir\n</code></pre>"},{"location":"neogpt/","title":"Run NeoGPT \u26a1\ufe0f","text":""},{"location":"neogpt/#cli-usage","title":"CLI Usage","text":"<p>Run the CLI to start using NeoGPT. The below command will run NeoGPT with default options.</p> Terminal<pre><code>python main.py\n</code></pre> <p>Note</p> <p>On first run, NeoGPT will download the model from HuggingFace and build the database. This may take few minutes.</p> <p>Once everything is set up, you will be greeted with the following message:</p> <p></p> <p>There are various options available to run the CLI. You can use the <code>--help</code> flag to view the available commands and options.</p> Terminal<pre><code>python main.py --help\n</code></pre>"},{"location":"neogpt/#cli-options","title":"CLI Options","text":"<ol> <li> <p><code>--ui</code>: You can use <code>--ui</code> flag to run the Streamlit UI. Terminal<pre><code>python main.py --ui\n</code></pre></p> </li> <li> <p><code>--db</code>: You can also use <code>--db</code> flag to specify the database to use. Currently the supported databases are:</p> <ul> <li> <p><code>Chroma</code> (default)</p> </li> <li> <p><code>FAISS</code> Terminal<pre><code>python main.py --db FAISS\n</code></pre></p> </li> </ul> </li> <li> <p><code>--persona</code>: You can also use <code>--persona</code> flag to specify the persona to use. Currently the supported personas are:</p> <ul> <li> <p><code>DEFAULT</code>: An helpful assistant that will help you with your queries. (default)</p> </li> <li> <p><code>RECRUITER</code>: An experienced recruiter who finds the best candidates.</p> </li> <li> <p><code>ACADEMICIAN</code>: Engages in in-depth research and presents findings.</p> </li> <li> <p><code>FRIEND</code>: Provides comfort and encouragement as a friend.</p> </li> <li> <p><code>ML_ENGINEER</code>: Explains complex ML concepts in an easy-to-understand manner.</p> </li> <li> <p><code>CEO</code>: Acts as the CEO, making strategic decisions.</p> </li> <li> <p><code>RESEARCHER</code>: Analyzes, synthesizes, and provides insights. Terminal<pre><code>python main.py --persona default\n</code></pre></p> </li> </ul> </li> </ol> <p>To know more about personas, refer here.</p> <ol> <li> <p><code>--retriever</code>: You can specify the retriever you want to use in the CLI. Currently the supported retrievers are:</p> <ul> <li> <p>Local Retriever (default)</p> </li> <li> <p>Web Research Retriever (Requires Google API Key refer here)</p> </li> <li> <p>Hybrid Retriever (Ensemble Retriever)</p> </li> <li> <p>SQL Retriever (Experimental)</p> </li> <li> <p>Context Compressor Retriever</p> </li> <li> <p>Stepback Prompting Retriever (RAG + DuckDuckGo Search + Stepback Prompting) Terminal<pre><code>python main.py --retriever local\n</code></pre></p> </li> </ul> <p>You can read more about retrievers in the retriever section.</p> </li> <li> <p><code>--build</code>: You can use <code>--build</code> flag to build the database. This will build the database using the files in <code>neogpt/documents</code> folder.     Basically it will run the <code>builder.py</code> script. You can read more about the builder here. Terminal<pre><code>python main.py --build\n</code></pre></p> </li> <li> <p><code>--show_source</code>: You can use <code>--show_source</code> flag to show the source of the retrieved document. This will show the documents that are retrieved from the database and fed into the LLM Terminal<pre><code>python main.py --show_source\n</code></pre></p> </li> <li> <p><code>--model_type</code> : You can use <code>--model_type</code> flag to specify how you want to load the model. Currently the supported LLM's are:</p> <ul> <li><code>LLamaCpp</code> (default)</li> <li><code>Ollama</code></li> <li><code>HuggingFace</code> Terminal<pre><code>python main.py --model_type mistral\n</code></pre></li> </ul> </li> <li> <p><code>--verbose</code>: You can use <code>--verbose</code> flag to enable verbose mode. This will print the logs to the terminal. Terminal<pre><code>python main.py --verbose\n</code></pre></p> </li> <li> <p><code>--debug</code>: You can use <code>--debug</code> flag to enable debug mode. This will print the debug logs to the terminal. Terminal<pre><code>python main.py --debug\n</code></pre></p> </li> <li> <p><code>--log</code>: You can use <code>--log</code> flag to log the output to a file. This will log the output to <code>logs/neogpt.log</code> file. Terminal<pre><code>python main.py --log\n</code></pre></p> </li> <li> <p><code>--recursive</code>: You can use <code>--recursive</code> flag to extract the child pages of a URL (for non-YouTube URLs). The builder script uses <code>WebBaseLoader</code> by default, which extracts only the root or specified domain of the URL. To extract child pages, run the builder script with the <code>--recursive</code> flag: Terminal<pre><code>python main.py --recursive\n</code></pre>     !!! warning \"Note\"         Use <code>--recursive</code> with the <code>--build</code> flag to extract child pages of a URL.</p> </li> <li> <p><code>--write</code> : You can use <code>--write</code> flag to write the output to a file. This will write the output to specified file.  Terminal<pre><code>python main.py --write output.txt\n</code></pre></p> </li> <li> <p><code>--version</code>: You can use <code>--version</code> flag to view the version of NeoGPT. Terminal<pre><code>python main.py --version\n</code></pre></p> </li> </ol>"},{"location":"neogpt/#streamlit-ui","title":"Streamlit UI","text":"<p>You can also use the Streamlit UI to interact with NeoGPT. To run the Streamlit UI, execute the following command in your terminal or command prompt:</p> Terminal<pre><code>python main.py --ui\n</code></pre> <p>This will start the Streamlit UI in your browser, if it doesn't open automatically, you can open it manually by going to <code>http://localhost:8501</code> in your browser.</p> <p></p> <p>Note</p> <p>The Streamlit UI is still in development. It only supports the <code>Local Retriever</code> for now.</p>"},{"location":"about/changelog/","title":"Change Log","text":"<p>This page lists the highlights, bug fixes and known issues for each release of NeoGPT.</p>"},{"location":"about/changelog/#v010-alpha","title":"v0.1.0-alpha \ud83d\ude80\ud83e\udd16\u2728","text":"<p>Release Date: 8<sup>th</sup> November 2023</p> <p>HighLights</p> <ul> <li> <p>Developed during Hacktoberfest 2023 \ud83c\udf83</p> </li> <li> <p>Open-source, locally-run Language Model (LLM) \ud83d\udcda</p> </li> <li> <p>Chat with documents, YouTube videos, and more \ud83c\udfa5\ud83d\udcc4\ud83d\udd75\ufe0f\u200d\u2642\ufe0f</p> </li> </ul> <p>Supported Retrievers:</p> <ul> <li> <p>Local Retriever</p> </li> <li> <p>Web Retriever</p> </li> <li> <p>Hybrid Retriever (Ensemble Retriever)</p> </li> <li> <p>SQL Retriever (Experimental)</p> </li> <li> <p>Context Compressor Retriever</p> </li> <li> <p>Stepback Prompting + RAG + DuckDuckGo Search</p> </li> </ul> <p>Project Roadmap Achievements:</p> <ul> <li> <p> RAG (Question Answering with local files) \ud83d\udcc2</p> </li> <li> <p> Chat with Youtube Videos \ud83c\udfa5</p> </li> <li> <p> Web Based RAG (Search on Web and local files) \ud83c\udf10\ud83d\udcc2</p> </li> <li> <p> Hybrid RAG (Keyword based and Semmantic Search) \ud83d\udd75\ufe0f\u200d\u2642\ufe0f\ud83d\udcc2</p> </li> <li> <p> FAISS Support \ud83d\udcca</p> </li> <li> <p> Chromadb Support \ud83c\udfb5</p> </li> <li> <p> Build a user-friendly CLI \u2328\ufe0f</p> </li> <li> <p> Upgrade Builder to support multiple file types including URLs \ud83d\udce6\ud83c\udf10</p> </li> <li> <p> User Interface \ud83d\udcbb (Streamlit)</p> </li> <li> <p> Chat with SQL DB \ud83e\udd16</p> </li> <li> <p> Support for other search engines (DuckDuckGo) \ud83d\udd0d</p> </li> </ul>"},{"location":"about/changelog/#contributors","title":"Contributors","text":"<ul> <li>@neokd</li> <li>@Priyamakeshwari</li> <li>@Gladwin001</li> <li>@qasim0014</li> <li>@Chandak-Keshav</li> <li>@fvaysh</li> <li>@Ymir-badam</li> <li>@Abhishekgupta204</li> <li>@bermr</li> <li>@dikshant182004</li> <li>@VPraharsha03</li> <li>@AndreasWintherMoen</li> <li>@vbhasin999</li> <li>@PentW0lf</li> <li>@savyez</li> <li>@y9rabbito</li> <li>@zakhaev26</li> <li>@ha36d</li> <li>@bryce-seefieldt</li> <li>@kehsihba19</li> </ul> <p>We would like to thank all the contributors for their valuable contributions to NeoGPT. </p>"},{"location":"about/code_of_conduct/","title":"Code of Conduct","text":"<p>NeoGPT is committed to fostering a welcoming and inclusive community. All participants in our community and contributors to our projects are expected to show respect and courtesy to others.</p> <ul> <li>Be Respectful: Treat everyone with kindness and respect.</li> <li>Be Inclusive: Welcome all backgrounds and perspectives; no discrimination or harassment.</li> <li>Be Collaborative: Work together constructively, resolve differences through discussion.</li> <li>Mind Your Language: Be mindful of your words; avoid offensive or disrespectful comments.</li> <li>No Trolling or Harassment: Don't troll, bully, or harass anyone.</li> <li>Respect Privacy: Don't share personal information without consent.</li> </ul> <p>Reporting Violations If you witness a violation, report it to us on GitHub Discussions or in our Discord community. We will investigate and take appropriate action.</p> <p>Thank you for upholding these principles in the NeoGPT community.</p>"},{"location":"about/contributing/","title":"Contributing to NeoGPT","text":"<p>We welcome contributions from the community to help improve and enhance our project. Please take a moment to review this document to understand how you can contribute effectively.</p>"},{"location":"about/contributing/#general-guidelines","title":"General Guidelines","text":"<ol> <li>Keep the code clean and readable.</li> <li>Follow the existing code style and formatting guidelines in the project to maintain consistency.</li> <li>Keep the feature small and focused. It is more likely to get merged if it is small and focused.</li> </ol>"},{"location":"about/contributing/#reporting-issues","title":"Reporting Issues","text":"<p>If you encounter a bug, have a feature request, or want to suggest an improvement, please open an issue on our GitHub issue tracker. When reporting an issue, be sure to provide as much as detail as possible, including</p> <ul> <li>Operating System.</li> <li>Version of Python.</li> <li>Detailed description of the problem.</li> <li>Examples for reproducing the error. You can post pictures, but if specific text or code is required to reproduce the issue, please provide the text in a plain text format for easy copy/paste.</li> </ul> <p>More the information you provide, easier it will be for us to reproduce the issue and fix it.</p>"},{"location":"about/contributing/#pull-requests","title":"Pull Requests","text":"<p>We encourage you to contribute code to NeoGPT through pull requests. To do so, please follow these steps:</p> <ol> <li> <p>Fork the NeoGPT repository to your own GitHub account.</p> </li> <li> <p>Clone the forked repository to your local machine.</p> </li> <li> <p>Create a new branch for your changes: <code>git checkout -b feature-name</code>.</p> </li> <li> <p>Implement your changes and commit them with meaningful commit messages.</p> </li> <li> <p>Push your changes to your forked repository.</p> </li> <li> <p>Create a pull request (PR) to the main NeoGPT repository, describing your changes and the problem they solve.</p> </li> </ol> <p>We will review your PR, provide feedback, and merge it if it meets our guidelines.</p>"},{"location":"about/contributing/#getting-help","title":"Getting Help","text":"<p>If you have questions or need assistance while contributing, feel free to reach out to us via our GitHub Discussions</p>"},{"location":"about/contributing/#contact-us","title":"Contact Us","text":"<p>Join our community to stay up to date with the latest developments and to share your feedback. Discord </p> <p>We look forward to your contributions! </p>"},{"location":"advance/configuration/","title":"NeoGPT Configuration","text":"<p>This document outlines various configuration settings for NeoGPT. You can customize these settings to suit your needs.</p> <p>Note</p> <p>The default configurations are good enough for most use cases. You can skip this document if you don't want to customize the configurations. We don't recommend changing the configurations unless you know what you are doing.</p> <p>Note</p> <p>The configuration file is located at <code>neogpt/config.py</code>. You can edit the file to change the configurations.</p>"},{"location":"advance/configuration/#directories","title":"Directories","text":"<ul> <li> <p>Source Directory: The directory where documents are stored.   Default: <code>neogpt/documents</code></p> </li> <li> <p>Model Directory: The directory for storing models from HuggingFace.   Default: <code>neogpt/models</code></p> </li> <li> <p>Parent Database Directory: The main database directory.   Default: <code>neogpt/db</code></p> </li> <li> <p>Chroma Database Directory: Directory for the Chroma database.   Default: <code>neogpt/db/chroma</code></p> </li> <li> <p>FAISS Database Directory: Directory for the FAISS database.   Default: <code>neogpt/db/faiss</code></p> </li> </ul>"},{"location":"advance/configuration/#memory-and-model","title":"Memory and Model","text":"<ul> <li> <p>Default Memory Key: The default memory key to remember chat history.   Default: <code>2</code></p> </li> <li> <p>Model Name: The name of the GGUF model.   Default: <code>TheBloke/Mistral-7B-Instruct-v0.1-GGUF</code></p> </li> <li> <p>Model File: The model file to use.   Default: <code>mistral-7b-instruct-v0.1.Q4_K_M.gguf</code></p> </li> <li> <p>Embedding Model: The default embedding model.   Default: <code>sentence-transformers/all-MiniLM-L12-v2</code></p> </li> </ul>"},{"location":"advance/configuration/#threads-and-device","title":"Threads and Device","text":"<ul> <li> <p>Ingest Threads: The number of threads for document ingestion.   Default: <code>8</code></p> </li> <li> <p>Max Token Length: The maximum token length for the model.   Default: <code>8192</code></p> </li> <li> <p>Number of GPU Layers: Number of layers for GPU compatibility.   Default: <code>40</code>   MPS: <code>1</code></p> </li> <li> <p>Device Type: The device type (cpu, mps, cuda).   Default: <code>cuda</code></p> </li> </ul>"},{"location":"advance/configuration/#chroma-database-settings","title":"Chroma Database Settings","text":"<ul> <li>Chroma Settings: Anonymized telemetry and persistence settings.<ul> <li>Anonymized Telemetry: <code>False</code></li> <li>Is Persistent: <code>True</code></li> </ul> </li> </ul>"},{"location":"advance/configuration/#reserved-file-names","title":"Reserved File Names","text":"<ul> <li>List of reserved file names.   Default: <code>[\"builder.url\"]</code></li> </ul>"},{"location":"advance/configuration/#supported-document-extensions","title":"Supported Document Extensions","text":"<ul> <li> <p>Supported document formats and their corresponding loaders.   Default:</p> </li> <li> <p><code>.pdf</code>: PDFMinerLoader</p> </li> <li> <p><code>.txt</code>: TextLoader</p> </li> <li> <p><code>.csv</code>: CSVLoader</p> </li> <li>... (other formats)</li> </ul> <p>Read more about the supported document formats here.</p>"},{"location":"advance/configuration/#url-extensions","title":"URL Extensions","text":"<p>Supported URL patterns for ingestion.</p> <ul> <li><code>.youtube</code>: YoutubeLoader</li> </ul>"},{"location":"advance/configuration/#query-and-cost","title":"Query and Cost","text":"<ul> <li> <p>Initial Query Cost: Initial query cost.   Default: <code>0</code></p> </li> <li> <p>Total Cost: Total cost.   Default: <code>0</code></p> </li> </ul>"},{"location":"advance/configuration/#logging","title":"Logging","text":"<ul> <li> <p>Log Folder: The folder for log files.   Default: <code>logs</code></p> </li> <li> <p>Log File: The log file name.   Default: <code>logs/builder.log</code></p> </li> </ul> <p>Release Note</p> <p>You cannot change config directly from the CLI or UI. We will add support for that in the future releases.</p>"},{"location":"advance/search/","title":"Search","text":""},{"location":"advance/search/#google-api-key-configuration","title":"Google API Key Configuration","text":"<p>Note</p> <p>You need to have a Google API key to use Web Research Retriever.</p>"},{"location":"advance/search/#follow-the-steps-below-to-get-your-google-api-key","title":"Follow the steps below to get your Google API key:","text":"<ol> <li> <p>You can find a <code>.env-template</code> file in the root directory of the repository. Rename it to <code>.env</code>.</p> </li> <li> <p>Go to Google Cloud Console and create a new project.</p> </li> <li> <p>If you don't have an account, you will be asked to create one and log in.</p> </li> <li> <p>Create a new project by clicking on the Select a Project dropdown at the top of the page and clicking New Project.</p> </li> <li> <p>Give your project a name and click Create.</p> </li> <li> <p>Set up a custom search API.</p> <ol> <li> <p>Go to the API and Services page.</p> </li> <li> <p>Click on Enable APIs and Services.</p> </li> <li> <p>Search for Custom Search API and click on it and then click Enable.</p> </li> <li> <p>Click on Create Credentials and select API Key.</p> </li> <li> <p>Set is as GOOGLE_API_KEY in the <code>.env</code> file.</p> </li> </ol> </li> <li> <p>Set up a custom search engine. By Enabling. Set up a custom search engine and add to your <code>.env</code> file.</p> <ol> <li> <p>Go to the Custom Search Engine page.</p> </li> <li> <p>Click on Add. You can setup the search engine by following the instructions. You can set to search the entire web or only specific sites based on your needs.</p> </li> <li> <p>Once you have created the search engine, click on Control Panel and then click on Basics. Copy the Search engine ID and set it as GOOGLE_CSE_ID in the <code>.env</code> file.</p> </li> </ol> </li> </ol> <p>Warning</p> <p>Avoid committing your <code>.env</code> file to the repository. It contains sensitive information.</p>"},{"location":"models/llamacpp/","title":"NeoGPT + LLamaCpp","text":"<p>NeoGPT uses LLamaCpp to load the LLM models. LLamaCpp is a python binding for llama.cpp. It supports GGUF format to load the LLM models. This enhances the speed and effectiveness of NeoGPT in handling large language models, contributing to a more efficient natural language processing experience.</p>"},{"location":"models/llamacpp/#how-to-use","title":"How to Use","text":"Terminal<pre><code>python main.py --model_type llama\n</code></pre> <p>By default, NeoGPT uses the <code>Mistral-7B instruct</code> model. In order to use other models, follow the instructions below.</p>"},{"location":"models/llamacpp/#using-other-models","title":"Using Other Models","text":"<p>Run the following command in your terminal or command prompt:</p> TerminalExample Terminal<pre><code>export MODEL_NAME=\"example_model\" &amp;&amp; export  MODEL_FILE=\"example_model.gguf\"\n</code></pre> Terminal<pre><code>export MODEL_NAME=\"TheBloke/Llama-2-7B-Chat-GGUF\" &amp;&amp; export  MODEL_FILE=\"llama-2-7b-chat.Q4_0.gguf\"\n</code></pre> <p>Make sure to replace <code>example_model</code> with the name of the model you want to use and <code>example_model.gguf</code> with the name of the model file.</p>"},{"location":"models/llamacpp/#tested-models-q4","title":"Tested Models (Q4)","text":"Model Name Model Size Tested Mistral-7B instruct 4.1GB \u2705 Llama 2 &amp;B 3.83 \u2705"},{"location":"models/ollama/","title":"NeoGPT + Ollama","text":"<p>Ollama allows you to run LLM'S locally. It basically bundles model weights, configuration, and data into a single package, defined by a Modelfile.They have a wide range of models available for download. You can download the models from here. </p> <p>Note</p> <p>It only supports MacOS and Linux for now. Windows support is coming soon. </p>"},{"location":"models/ollama/#how-to-use","title":"How to Use","text":"<p>Follow the steps below to use Ollama with NeoGPT.</p> <ul> <li> <p>First, you need to download Ollama. </p> </li> <li> <p>Fetch the model you want to use from here. Using <code>ollama pull &lt;model family&gt;</code></p> </li> <li> <p>This will download the most basic version of the model typically (e.g., smallest # parameters and q4_0)</p> </li> <li> <p>If the app is running.All of your local models are automatically served on <code>localhost:11434</code></p> </li> <li> <p>Set the environment variable <code>MODEL_NAME</code> to the name of the model you want to use.</p> </li> </ul> CommandExample Terminal<pre><code>export MODEL_NAME=\"example_model\"\n</code></pre> Terminal<pre><code>python main.py --model_type ollama\n</code></pre> <p>Make sure to replace <code>example_model</code> with the name of the model you want to use. It will be the model you choose from Ollama Library.</p>"},{"location":"models/ollama/#tested-models-q4","title":"Tested Models (Q4)","text":"Model Name Model Size Tested Mistral-7B instruct 4.1GB \u2705 Llama 2 &amp;B 3.83 \u2705"},{"location":"persona/persona/","title":"Persona &amp; NeoGPT","text":""},{"location":"persona/persona/#what-is-persona","title":"What is Persona?","text":"<p>In the world of artificial intelligence, there's a fascinating quest to make chatbots more interesting and responsive by incorporating personas. NeoGPT takes a creative step in this direction. Think of a persona as a character that NeoGPT takes on to better understand and respond to different situations. It's like giving the model different roles to play, making your interactions more diverse and engaging. NeoGPT supports multiple personas, each with its own role and style, adding a special touch to how it chats with you.</p> <p>Warning</p> <p>The persona feature is still in beta. We are working on adding more personas and improving the existing ones. Persona's will improve with agents in the future.If you have any suggestions or feedback, please let us know here</p>"},{"location":"persona/persona/#available-personas","title":"Available Personas","text":"<p>The following personas are currently available:</p> <ul> <li><code>DEFAULT</code>: An helpful assistant that will help you with your queries. (default)</li> <li><code>RECRUITER</code>: An experienced recruiter who finds the best candidates.</li> <li><code>ACADEMICIAN</code>: Engages in in-depth research and presents findings.</li> <li><code>FRIEND</code>: Provides comfort and encouragement as a friend.</li> <li><code>ML_ENGINEER</code>: Explains complex ML concepts in an easy-to-understand manner.</li> <li><code>CEO</code>: Acts as the CEO, making strategic decisions.</li> <li><code>RESEARCHER</code>: Analyzes, synthesizes, and provides insights.</li> </ul>"},{"location":"persona/persona/#default-persona","title":"Default Persona","text":"<p>Imagine NeoGPT as your ever-reliable assistant. In its default persona, NeoGPT diligently processes the information you provide, responding with thoughtful and step-by-step answers. It doesn't just answer questions; it guides you through the thought process, making your interaction more informative and engaging.</p> Terminal<pre><code>python main.py --persona default\n</code></pre> <p>Tip</p> <p>This is the default persona. You don't need to specify it in the command.</p>"},{"location":"persona/persona/#recruiter-persona","title":"Recruiter Persona","text":"<p>NeoGPT can also act as a recruiter, helping you find the best candidates for your company. In this persona, NeoGPT asks you a series of questions to understand your requirements and then uses that information to find the best candidates for the job. It is a great way to find the right people for your company and also allowing candidates to showcase their skills.</p> Terminal<pre><code>python main.py --persona recruiter\n</code></pre>"},{"location":"persona/persona/#academician-persona","title":"Academician Persona","text":"<p>NeoGPT can also act as an academician, helping you find the best research papers for your research. In this persona, NeoGPT asks you a series of questions to understand your requirements and then uses that information to find the best research papers for your research. It is a great way to find the right research papers for your research and also allowing researchers to showcase their skills.</p> Terminal<pre><code>python main.py --persona academician\n</code></pre>"},{"location":"persona/persona/#friend-persona","title":"Friend Persona","text":"<p>NeoGPT's friend persona brings a warm and empathetic touch to your conversations. No need for explanations \u2013 this persona is here to offer unwavering support, providing comforting words and positive vibes during life's challenges.</p> Terminal<pre><code>python main.py --persona friend\n</code></pre>"},{"location":"persona/persona/#ml-engineer-persona","title":"ML Engineer Persona","text":"<p>Tech enthusiasts, rejoice! NeoGPT steps into the shoes of a machine learning engineer, breaking down complex concepts into easy-to-understand terms. Expect step-by-step explanationsand recommendations for further study \u2013 a true tech guide..</p> <p>Tip</p> <p>Perfect for beginners who want to learn more about machine learning. Combine this persona with the test data to get the best results.</p> Terminal<pre><code>python main.py --persona ml_engineer\n</code></pre>"},{"location":"persona/persona/#ceo-persona","title":"CEO Persona","text":"<p>NeoGPT as a CEO brings strategic thinking to the forefront. Handling challenges, making big decisions, and representing the company \u2013 this persona showcases leadership skills, offering insights into the corporate world.</p> Terminal<pre><code>python main.py --persona ceo\n</code></pre>"},{"location":"persona/persona/#researcher-persona","title":"Researcher Persona","text":"<p>For data exploration and analysis, NeoGPT becomes a researcher. Dive into the world of data with this persona, where NeoGPT sifts through information, analyzes trends and patterns, and presents its findings in a clear and concise manner.</p> Terminal<pre><code>python main.py --persona researcher\n</code></pre>"},{"location":"persona/persona/#conclusion","title":"Conclusion","text":"<p>In essence, NeoGPT's personas transform your chatting experience into a multifaceted journey. Whether you're seeking assistance, strategizing, exploring data, or just having a friendly chat, NeoGPT adapts seamlessly, making it your all-in-one conversational companion.</p>"},{"location":"retrievers/context/","title":"Contextual Compressor Retriever","text":""},{"location":"retrievers/context/#description","title":"Description","text":"<p>The Contextual Compression Retriever is a crucial component within NeoGPT, designed to enhance the efficiency of document retrieval. It optimizes the information presented to a Large Language Model (LLM) by compressing and filtering documents based on the context of a user's query.</p> <p>Initially, a Base Retriever fetches relevant documents from a storage system, and these, along with the user's query, are then processed by the LLM. Subsequently, a Document Compressor refines the LLM's output, shortening and filtering documents to create a set of contextually compressed documents. This streamlined information is then seamlessly integrated into the application, ensuring users receive concise and relevant responses aligned with the nuances of their original queries.</p> <p><pre><code>graph TD\n  A[User Query] --&gt;|Query| B[Base Retriever]\n  B --&gt;|Retrieved Documents| C[LLM]\n  C --&gt;|User Query &amp; Retrieved Docs| D[Document Compressor]\n  D --&gt;|Compressed Documents| E[Answer]\n\n  D --&gt;|Query| F[Base Retriever]\n  F --&gt;|Retrieved Documents| G[Document Compressor]\n  G --&gt;|Compressed Documents| D\n</code></pre> Figure 1: Illustration of Contextual Compressor Retriever in NeoGPT.</p> <p>The Contextual Compression Retriever offers notable advantages, enhancing system efficiency, improving the relevance of retrieved content, and optimizing the utilization of the LLM, making it a valuable asset in retrieval-based natural language processing.</p>"},{"location":"retrievers/context/#how-to-use","title":"How to Use","text":"<p>Run the following command to interact with the Contextual Compressor Retriever:</p> Terminal<pre><code>python main.py --retriever compressor\n</code></pre>"},{"location":"retrievers/hybrid/","title":"Hybrid Retriever","text":""},{"location":"retrievers/hybrid/#description","title":"Description","text":"<p>The Ensemble Retriever in NeoGPT combines the strengths of different retrieval algorithms to enhance document search. By integrating both sparse retrievers, such as BM25 for keyword-based searches, and dense retrievers, like embedding similarity for semantic understanding, it achieves superior performance compared to individual algorithms. </p> <p>This hybrid approach ensures a comprehensive retrieval process, where keyword relevance and semantic context work harmoniously, delivering more accurate and nuanced search results. In essence, the Ensemble Retriever optimizes document retrieval by leveraging the unique advantages of diverse algorithms, resulting in a powerful and adaptive search experience. The hybrid retriever is also known as ensemble retriever. </p> <p><pre><code>\ngraph TD\n  A[User Query] --&gt;|Query| B[EnsembleRetriever]\n  B --&gt;|Get Relevant Documents| C[Sparse Retriever]\n  B --&gt;|Get Relevant Documents| D[Dense Retriever]\n  C --&gt;|Ranked Documents| E[Ensemble Retriever]\n  D --&gt;|Ranked Documents| E\n  E --&gt;|Combined Documents| F[LLM]\n  F --&gt;|Generate Response| G[Answer]</code></pre> Figure 1: Illustration of Ensemble Retriever in NeoGPT.</p>"},{"location":"retrievers/hybrid/#how-to-use","title":"How to Use","text":"<p>Run the following command to interact with the Ensemble Retriever:</p> Terminal<pre><code>python main.py --retriever hybrid\n</code></pre>"},{"location":"retrievers/local/","title":"Local Retriever","text":""},{"location":"retrievers/local/#description","title":"Description","text":"<p>The Local Retriever is a basic retriever that retrieves documents from the local vector database and does not reply on the internet. It is the default retriever that is used in NeoGPT. It pulls documents from the local vector database and feeds them into the LLM. It is the fastest and simple retriever among all the retrievers.</p> <pre><code>graph TD\n  A[User Query] --&gt; B[Local Retriever]\n  B --&gt;|Retrieved Documents| C[LLM]\n  C --&gt;|User Query &amp; Retrieved Docs| D[Answer]</code></pre> <p>Figure 1: Illustration of Local Retriever in NeoGPT.</p>"},{"location":"retrievers/local/#how-to-use","title":"How to Use","text":"<p>Run the following command to interact with the Local Retriever:</p> Basic CommandLocal Retriever Terminal<pre><code>python main.py \n</code></pre> Terminal<pre><code>python main.py --retriever local\n</code></pre>"},{"location":"retrievers/sql/","title":"SQL Retriever","text":""},{"location":"retrievers/sql/#description","title":"Description","text":"<p>NeoGPT facilitates smooth communication between users and SQL databases through natural language. Typically, data resides in SQL databases, and this retriever empowers users to interact with a SQL database effortlessly using Large Language Models (LLMs). It dynamically constructs and executes SQL queries based on the output generated by the LLM, enhancing the user experience in querying and interacting with database information.</p> <p>Supported SQL databases: - MySQL</p> <ul> <li> <p>PostgreSQL</p> </li> <li> <p>SQLite</p> </li> <li> <p>Oracle SQL</p> </li> <li> <p>Databricks</p> </li> </ul> <p><pre><code>graph TD\n  A[User's Complex Question] --&gt;|LLM| B[Query Generation]\n  B --&gt;|Database Interaction | C[Database Output]\n  C --&gt;|LLM| D[Answer to User's Question]\n\n    style B stroke:#FF9800\n    style C stroke:#FF9800</code></pre> Figure 1: Illustration of SQL Retriever in NeoGPT.</p> <p>We leverage Langchain's SQL retriever in this project. For additional details, please refer here.</p>"},{"location":"retrievers/sql/#how-to-use","title":"How to Use","text":"<p>Builder\ud83d\udc77 Warning</p> <p>The SQL retriever is not supported by the builder and does not rely on any of the builder's components or the vector database. It is a standalone retriever that can be used independently.</p> <p>To interact with a SQL database, follow the steps below:</p> <ul> <li>Move the SQL database file to the documents folder (Example:  <code>neogpt/documents/</code>).</li> <li>Run the following command to interact with the SQL database:</li> </ul> Terminal<pre><code>python main.py --retriever sql\n</code></pre> <p>After running the command, you will be prompted to enter a question. Enter a question and press <code>Enter</code>. The SQL retriever will generate a SQL query based on the question and execute it on the SQL database. The output will be displayed on the terminal.</p>"},{"location":"retrievers/stepback/","title":"Stepback Retriever","text":""},{"location":"retrievers/stepback/#description","title":"Description","text":"<p>Step-Back Prompting is a strategic approach tailored for Large Language Models (LLMs) to navigate complex tasks laden with intricate details. It involves a two-step process: first, guiding the LLM to formulate a higher-level, generic question about fundamental concepts; and second, leveraging the facts derived from this abstraction to reason effectively about the original, more detailed question. This method minimizes errors in intermediate steps, enhancing the LLM's ability to retrieve and reason over relevant information accurately.</p> <p><pre><code>graph TD\n  A[User's Complex Question] --&gt;|Step-Back Prompting| B[Higher-Level Search Query]\n  B --&gt;|Search on Internet| C[Retrieved Context]\n  C --&gt;|Contextual Understanding| D[Answer Higher-Level Query]\n  D --&gt;|Abstraction-Grounded Reasoning| E[Detailed Answer]\n\n  style A stroke:#4CAF50\n  style B stroke:#FF9800\n  style C stroke:#2196F3\n  style D stroke:#FF9800\n  style E stroke:#4CAF50</code></pre> Figure 1: Illustration of Step-Back Prompting in NeoGPT.</p> <p>The Stepback Retriever, inspired by a prompting technique detailed in the Google DeepMind paper, utilizes these principles to optimize information retrieval and enhance the overall performance of the LLM in tackling challenging tasks. In essence, Step-Back Prompting with Stepback Retriever provides a robust framework for addressing complex queries by strategically navigating through higher-level abstractions.</p>"},{"location":"retrievers/stepback/#how-to-use","title":"How to Use","text":"<p>Builder\ud83d\udc77 Warning</p> <p>The Stepback Retriever is not supported by the builder and does not rely on any of the builder's components or the vector database. It is a standalone retriever that can be used independently. It uses Duck Duck Go as the search engine to retrieve relevant documents from the internet.</p> <p>Run the following command to interact with the Stepback Retriever:</p> Terminal<pre><code>python main.py --retriever stepback\n</code></pre>"},{"location":"retrievers/web/","title":"Web Research Retriever","text":""},{"location":"retrievers/web/#description","title":"Description","text":"<p>The Web Research Retriever seamlessly integrates internet and local database sources to fetch documents for NeoGPT. Leveraging the Google Custom Search API, it performs internet searches and retrieves data from the local vector database. The amalgamated information is then fed into NeoGPT's Language Model (LLM). This retriever is ideal for obtaining the latest internet data and retrieving information from the local vector database.</p> <p>Setting up the Web Research Retriever requires a Google API key and a Google Custom Search Engine ID. For detailed instructions, please refer here.</p> <pre><code>\ngraph TD\n  A[User Query] --&gt;|Query| B[Formulate Google Searches]\n  B --&gt;|List of Searches| C[Search Using Google Custom Search API]\n  C --&gt;|Search Results| D[Load Resulting URLs]\n  D --&gt;|Page Contents| E[Embed Content]\n  E --&gt;|Similarity Search with Query| F[Consolidate Relevant Documents]\n  F --&gt;|Consolidated Documents| G[LLM]\n  G --&gt;|User Query &amp; Retrieved Docs| H[Answer]\n\n  B --&gt;|Query| I[Local Retriever]\n  I --&gt;|Retrieved Documents| F[Consolidate Relevant Documents]\n\n    style C stroke:#FF9800\n    style D stroke:#FF9800\n    style E stroke:#FF9800\n    style F stroke:#FF9800</code></pre>"},{"location":"retrievers/web/#how-to-use","title":"How to Use","text":"<p>Run the following command to interact with the Web Research Retriever:</p> Terminal<pre><code>python main.py --retriever web\n</code></pre>"}]}